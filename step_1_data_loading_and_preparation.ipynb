{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_LABELS = {\n",
    "    1: \"sitting\",\n",
    "    2: \"standing\",\n",
    "    3: \"lying on back\",\n",
    "    4: \"lying on right side\",\n",
    "    5: \"ascending stairs\",\n",
    "    6: \"descending stairs\",\n",
    "    7: \"standing in elevator still\",\n",
    "    8: \"moving in elevator\",\n",
    "    9: \"walking in parking lot\",\n",
    "    10: \"walking on treadmill (flat)\",\n",
    "    11: \"walking on treadmill (inclined)\",\n",
    "    12: \"running on treadmill\",\n",
    "    13: \"exercising on stepper\",\n",
    "    14: \"exercising on cross trainer\",\n",
    "    15: \"cycling (horizontal)\",\n",
    "    16: \"cycling (vertical)\",\n",
    "    17: \"rowing\",\n",
    "    18: \"jumping\",\n",
    "    19: \"playing basketball\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "def get_column_names():\n",
    "    \"\"\"Generate column names for the sensor data.\"\"\"\n",
    "    units = ['T', 'RA', 'LA', 'RL', 'LL']\n",
    "    sensors = ['acc_x', 'acc_y', 'acc_z', \n",
    "              'gyro_x', 'gyro_y', 'gyro_z',\n",
    "              'mag_x', 'mag_y', 'mag_z']\n",
    "    \n",
    "    columns = []\n",
    "    for unit in units:\n",
    "        for sensor in sensors:\n",
    "            columns.append(f\"{unit}_{sensor}\")\n",
    "    \n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_segment_file(file_path):\n",
    "    \"\"\"Load a single segment file and return as DataFrame with proper column names.\"\"\"\n",
    "    try:\n",
    "        # Read the CSV file without headers\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        \n",
    "        # Get just the sensor column names (45 columns)\n",
    "        sensor_columns = get_column_names()\n",
    "        \n",
    "        # Assign the sensor column names to the DataFrame\n",
    "        if len(df.columns) != len(sensor_columns):\n",
    "            print(f\"Warning: {file_path} has {len(df.columns)} columns, expected {len(sensor_columns)}\")\n",
    "            return None\n",
    "            \n",
    "        df.columns = sensor_columns\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def collect_all_data(root_dir):\n",
    "    \"\"\"\n",
    "    Collect all sensor data from the directory structure.\n",
    "    Returns a DataFrame with all data and corresponding labels.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Loop through all activities (a01-a19)\n",
    "    for activity in tqdm(range(1, 20), desc=\"Processing activities\"):\n",
    "        activity_dir = os.path.join(root_dir, f'a{activity:02d}')\n",
    "        \n",
    "        # Loop through all persons (p1-p8)\n",
    "        for person in range(1, 9):\n",
    "            person_dir = os.path.join(activity_dir, f'p{person}')\n",
    "            \n",
    "            # Loop through all segments (s01-s60)\n",
    "            for segment in range(1, 61):\n",
    "                segment_file = os.path.join(person_dir, f's{segment:02d}.txt')\n",
    "                \n",
    "                # Load the segment data\n",
    "                segment_data = load_segment_file(segment_file)\n",
    "                if segment_data is not None:\n",
    "                    # Add metadata after loading the sensor data\n",
    "                    segment_data['activity'] = activity\n",
    "                    segment_data['person'] = person\n",
    "                    segment_data['segment'] = segment\n",
    "                    segment_data['activity_name'] = ACTIVITY_LABELS[activity]\n",
    "                    \n",
    "                    # Add to collection\n",
    "                    all_data.append(segment_data)\n",
    "    \n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_data(root_dir):\n",
    "    \"\"\"\n",
    "    Collect all sensor data from the directory structure.\n",
    "    Returns a DataFrame with all data and corresponding labels.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Loop through all activities (a01-a19)\n",
    "    for activity in tqdm(range(1, 20), desc=\"Processing activities\"):\n",
    "        activity_dir = os.path.join(root_dir, f'a{activity:02d}')\n",
    "        \n",
    "        # Loop through all persons (p1-p8)\n",
    "        for person in range(1, 9):\n",
    "            person_dir = os.path.join(activity_dir, f'p{person}')\n",
    "            \n",
    "            # Loop through all segments (s01-s60)\n",
    "            for segment in range(1, 61):\n",
    "                segment_file = os.path.join(person_dir, f's{segment:02d}.txt')\n",
    "                \n",
    "                # Load the segment data\n",
    "                segment_data = load_segment_file(segment_file)\n",
    "                if segment_data is not None:\n",
    "                    # Add metadata\n",
    "                    segment_data['activity'] = activity\n",
    "                    segment_data['person'] = person\n",
    "                    segment_data['segment'] = segment\n",
    "                    \n",
    "                    # Add to collection\n",
    "                    all_data.append(segment_data)\n",
    "    \n",
    "    # Combine all data into a single DataFrame\n",
    "    combined_data = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_activity_data(data_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Main function to prepare the activity classification dataset.\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"Starting data preparation...\")\n",
    "    \n",
    "    # 1. Collect all data\n",
    "    print(\"Collecting data from all segments...\")\n",
    "    full_dataset = collect_all_data(data_dir)\n",
    "    \n",
    "    # 2. Basic statistics and info\n",
    "    print(\"\\nDataset Overview:\")\n",
    "    print(f\"Total samples: {len(full_dataset)}\")\n",
    "    print(f\"Features per sample: {len(full_dataset.columns) - 3}\")  # Excluding metadata columns\n",
    "    print(\"\\nActivity distribution:\")\n",
    "    print(full_dataset['activity'].value_counts().sort_index())\n",
    "    \n",
    "    # 3. Save the full dataset\n",
    "    output_file = os.path.join(output_dir, 'activity_dataset.csv')\n",
    "    full_dataset.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDataset saved to: {output_file}\")\n",
    "    \n",
    "    # 4. Create a metadata file\n",
    "    metadata = {\n",
    "        'total_samples': len(full_dataset),\n",
    "        'features_per_sample': len(full_dataset.columns) - 3,\n",
    "        'activities': full_dataset['activity'].nunique(),\n",
    "        'persons': full_dataset['person'].nunique(),\n",
    "        'segments_per_activity': full_dataset.groupby('activity').size().to_dict()\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(output_dir, 'dataset_metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Collecting data from all segments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing activities: 100%|██████████| 19/19 [00:53<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Overview:\n",
      "Total samples: 1140000\n",
      "Features per sample: 45\n",
      "\n",
      "Activity distribution:\n",
      "activity\n",
      "1     60000\n",
      "2     60000\n",
      "3     60000\n",
      "4     60000\n",
      "5     60000\n",
      "6     60000\n",
      "7     60000\n",
      "8     60000\n",
      "9     60000\n",
      "10    60000\n",
      "11    60000\n",
      "12    60000\n",
      "13    60000\n",
      "14    60000\n",
      "15    60000\n",
      "16    60000\n",
      "17    60000\n",
      "18    60000\n",
      "19    60000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset saved to: processed_data\\activity_dataset.csv\n",
      "\n",
      "Processed Dataset Sample:\n",
      "   T_acc_x  T_acc_y  T_acc_z  T_gyro_x  T_gyro_y  T_gyro_z  T_mag_x   T_mag_y  \\\n",
      "0   8.1305   1.0349   5.4217 -0.009461  0.001915 -0.003424 -0.78712 -0.069654   \n",
      "1   8.1305   1.0202   5.3843 -0.009368  0.023485  0.001953 -0.78717 -0.068275   \n",
      "2   8.1604   1.0201   5.3622  0.015046  0.014330  0.000204 -0.78664 -0.068277   \n",
      "3   8.1603   1.0052   5.3770  0.006892  0.018045  0.005649 -0.78529 -0.069849   \n",
      "4   8.1605   1.0275   5.3473  0.008811  0.030433 -0.005346 -0.78742 -0.068796   \n",
      "\n",
      "   T_mag_z  RA_acc_x  ...  LL_acc_z  LL_gyro_x  LL_gyro_y  LL_gyro_z  \\\n",
      "0  0.15730   0.70097  ...    2.6220  -0.000232  -0.012092  -0.004457   \n",
      "1  0.15890   0.71829  ...    2.6218  -0.014784  -0.016477   0.002789   \n",
      "2  0.15879   0.69849  ...    2.6366  -0.012770   0.005717  -0.007918   \n",
      "3  0.15912   0.72799  ...    2.6070  -0.005725   0.009620   0.006555   \n",
      "4  0.15916   0.71572  ...    2.6218  -0.003929  -0.008371   0.002816   \n",
      "\n",
      "   LL_mag_x  LL_mag_y  LL_mag_z  activity  person  segment  \n",
      "0   0.74017   0.30053 -0.057730         1       1        1  \n",
      "1   0.73937   0.30183 -0.057514         1       1        1  \n",
      "2   0.73955   0.30052 -0.057219         1       1        1  \n",
      "3   0.74029   0.30184 -0.057750         1       1        1  \n",
      "4   0.73845   0.30090 -0.057527         1       1        1  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "\n",
      "Feature Names:\n",
      "['T_acc_x', 'T_acc_y', 'T_acc_z', 'T_gyro_x', 'T_gyro_y', 'T_gyro_z', 'T_mag_x', 'T_mag_y', 'T_mag_z', 'RA_acc_x', 'RA_acc_y', 'RA_acc_z', 'RA_gyro_x', 'RA_gyro_y', 'RA_gyro_z', 'RA_mag_x', 'RA_mag_y', 'RA_mag_z', 'LA_acc_x', 'LA_acc_y', 'LA_acc_z', 'LA_gyro_x', 'LA_gyro_y', 'LA_gyro_z', 'LA_mag_x', 'LA_mag_y', 'LA_mag_z', 'RL_acc_x', 'RL_acc_y', 'RL_acc_z', 'RL_gyro_x', 'RL_gyro_y', 'RL_gyro_z', 'RL_mag_x', 'RL_mag_y', 'RL_mag_z', 'LL_acc_x', 'LL_acc_y', 'LL_acc_z', 'LL_gyro_x', 'LL_gyro_y', 'LL_gyro_z', 'LL_mag_x', 'LL_mag_y', 'LL_mag_z', 'activity', 'person', 'segment']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define paths\n",
    "    data_dir = \"daily_and_sports_activities/data\"\n",
    "    output_dir = \"processed_data\"\n",
    "    \n",
    "    # Prepare the dataset\n",
    "    dataset = prepare_activity_data(data_dir, output_dir)\n",
    "    \n",
    "    # Display some basic information about the processed dataset\n",
    "    print(\"\\nProcessed Dataset Sample:\")\n",
    "    print(dataset.head())\n",
    "    \n",
    "    print(\"\\nFeature Names:\")\n",
    "    print(dataset.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
