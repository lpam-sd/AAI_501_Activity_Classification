{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "def load_and_preprocess_data(data_path, window_size=125):\n",
    "    \"\"\"\n",
    "    Load and preprocess the activity data with sliding windows\n",
    "    window_size=125 represents 5 seconds of data (25Hz * 5s = 125 samples)\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = df.drop(['activity', 'person', 'segment'], axis=1)\n",
    "    y = df['activity']\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Scale the features\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Save the scaler for later use\n",
    "    joblib.dump(scaler, 'processed_data/scaler.pkl')\n",
    "    \n",
    "    return X_scaled, y, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_sets(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets\n",
    "    \"\"\"\n",
    "    print(\"Splitting data into train and test sets...\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y  # Ensure balanced split across activities\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(X, y, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create a generator to yield batches of data\n",
    "    Useful for handling large datasets\n",
    "    \"\"\"\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i + batch_size]\n",
    "            yield X[batch_indices], y[batch_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Scaling features...\n",
      "Splitting data into train and test sets...\n",
      "\n",
      "Dataset Information:\n",
      "Training samples: 912000\n",
      "Testing samples: 228000\n",
      "Number of features: 45\n",
      "Number of classes: 19\n",
      "\n",
      "Saving processed datasets...\n",
      "\n",
      "Processing complete! Files saved in 'processed_data' directory\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Paths\n",
    "    data_path = \"processed_data/activity_dataset.csv\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_scaled, y, feature_names = load_and_preprocess_data(data_path)\n",
    "    \n",
    "    # Split into train/test sets\n",
    "    X_train, X_test, y_train, y_test = prepare_train_test_sets(X_scaled, y)\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(\"\\nDataset Information:\")\n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    \n",
    "    # Save the processed datasets\n",
    "    print(\"\\nSaving processed datasets...\")\n",
    "    np.save('processed_data/X_train.npy', X_train)\n",
    "    np.save('processed_data/X_test.npy', X_test)\n",
    "    np.save('processed_data/y_train.npy', y_train)\n",
    "    np.save('processed_data/y_test.npy', y_test)\n",
    "    \n",
    "    # Save feature names\n",
    "    with open('processed_data/feature_names.txt', 'w') as f:\n",
    "        f.write('\\n'.join(feature_names))\n",
    "    \n",
    "    print(\"\\nProcessing complete! Files saved in 'processed_data' directory\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, feature_names\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, X_test, y_train, y_test, feature_names = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
